{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5 - Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore [Grad-CAM](https://arxiv.org/abs/1610.02391) for explaining predictions of image classification models. This can help us to gain (visual) insights about what makes a model predict a specific class label. Explainability techniques are important for better understanding model behaviour and analyse failure cases.\n",
    "\n",
    "We will start by implementing Grad-CAM and use it on our LeNet-like MNIST classification model. We then analyse a [VGG-16 model](https://arxiv.org/abs/1409.1556) pre-trained on the ImageNet dataset.\n",
    "\n",
    "**Objective:** Use Grad-CAM to analyse deep convolutional neural networks for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Google Colab uncomment the following line to install PyTorch Lightning\n",
    "# ! pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
    "from torchmetrics.functional import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) for handling the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir: str = './data', batch_size: int = 32, num_workers: int = 4, transform = transforms.ToTensor()):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transform        \n",
    "\n",
    "        self.test_set = MNIST(self.data_dir, train=False, transform=self.transform, download=True)\n",
    "        dev_set = MNIST(self.data_dir, train=True, transform=self.transform, download=True)\n",
    "        self.train_set, self.val_set = random_split(dev_set, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) for implementing the model and its training and testing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(LightningModule):\n",
    "    def __init__(self, input_dim: tuple[int, int] = (28,28), output_dim: int = 10, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # LeNet\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "            )        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, self.output_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first pass x through the conv layers\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # then pass linearised x through the fully connected layers\n",
    "        return self.fc(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def process_batch(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)        \n",
    "        acc = accuracy(preds, y, task='multiclass', num_classes=self.output_dim)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.process_batch(batch)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        if batch_idx == 0:\n",
    "            grid = torchvision.utils.make_grid(batch[0][0:16, ...], nrow=4, normalize=True)\n",
    "            self.logger.experiment.add_image('train_images', grid, self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self.process_batch(batch)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self.process_batch(batch)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the LeNet-like model previously trained for MNIST image classification.\n",
    "\n",
    "**Task:** Add the path to your pre-trained MNIST classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)\n",
    "\n",
    "data = MNISTDataModule(data_dir='./data', batch_size=32)\n",
    "\n",
    "model_dir = '<path_to_model_checkpoint>' # for example: './lightning_logs/classification/mnist-lenet/version_0/checkpoints/epoch=5-step=10314.ckpt'\n",
    "model_dir = '../lightning_logs/classification/mnist-lenet/version_0/checkpoints/epoch=5-step=10314.ckpt'\n",
    "model = ImageClassifier.load_from_checkpoint(model_dir, input_dim=(28,28), output_dim=10)\n",
    "\n",
    "trainer = Trainer() # dummy trainer for running test() on the loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the pre-trained model on the test data and confirm the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class activation maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating class activation maps, we first need to decide which convolutional layer we want to use for this.\n",
    "\n",
    "Let's look at a print-out of the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ImageClassifier(\n",
    "  (conv): Sequential(\n",
    "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
    "    (4): ReLU()\n",
    "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (fc): Sequential(\n",
    "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
    "    (3): ReLU()\n",
    "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classification model consists of two sub-models, the convolutional part with two convolutional and two max-pooling layers, and the fully connected layers. For Grad-CAM, we usually want to compute the class activations at the output of last convolutional layer, before performing max-pooling.\n",
    "\n",
    "We now implement a modified model where separate out the application of the first five layers of the convolutional part, register the output for Grad-CAM calculations, and then apply the final max-pooling before passing the output to the fully connected layers. For this, we can modify the forward pass accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierCAM(ImageClassifier):\n",
    "    def __init__(self, input_dim: tuple[int, int] = (28,28), output_dim: int = 10, learning_rate: float = 0.001):\n",
    "        super().__init__(input_dim, output_dim, learning_rate)       \n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "\n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # disect the network to access its last convolutional layer\n",
    "        features_conv = self.conv[:5]\n",
    "\n",
    "        # first pass x through the conv layers\n",
    "        x = features_conv(x)\n",
    "        \n",
    "        # register the hook\n",
    "        x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # apply the remaining pooling\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        x = x.view((1, -1))        \n",
    "        # then pass linearised x through the fully connected layers\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        features_conv = self.conv[:5]\n",
    "        return features_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this modified model we can now run inference on a selected test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_modified = ImageClassifierCAM.load_from_checkpoint(model_dir, input_dim=(28,28), output_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch, _ = next(iter(data.test_dataloader()))\n",
    "test_batch = test_batch[0:16,...]\n",
    "\n",
    "grid = torchvision.utils.make_grid(test_batch, nrow=4, normalize=True).numpy()[0,...].squeeze()\n",
    "\n",
    "plt.imshow(grid, cmap=matplotlib.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.title('example test images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 1\n",
    "test_image = test_batch[test_index,...].unsqueeze(0)\n",
    "\n",
    "plt.imshow(test_image.squeeze(), cmap=matplotlib.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.title('test image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the modified model on the selected test image and store the output logits and the predicted class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model_modified(test_image)\n",
    "predicted_label = F.softmax(logits, dim=1).argmax(dim=1)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implements the Grad-CAM algorithm. For more details, please have a look at this [blog post](https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradcam(model, image, logits, class_label, num_feature_channels):\n",
    "    # get the gradient of the output with respect to the parameters of the model\n",
    "    logits[:, class_label].backward()\n",
    "\n",
    "    # pull the gradients out of the model\n",
    "    gradients = model.get_activations_gradient()\n",
    "\n",
    "    # pool the gradients across the channels\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "    # get the activations of the last convolutional layer\n",
    "    activations = model.get_activations(image).detach()\n",
    "\n",
    "    # weight the channels by corresponding gradients\n",
    "    for i in range(num_feature_channels):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "        \n",
    "    # average the channels of the activations\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "    # relu on top of the heatmap\n",
    "    # expression (2) in https://arxiv.org/abs/1610.02391\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "    # normalize the heatmap\n",
    "    heatmap /= torch.max(heatmap)\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run Grad-CAM and visualise the generated heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = run_gradcam(model=model_modified, image=test_image, logits=logits, class_label=predicted_label, num_feature_channels=16)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap, cmap=matplotlib.cm.bwr)\n",
    "plt.colorbar()\n",
    "plt.title('Grad-CAM heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap will have the same resolution as the output of the selected convolutional layer. In order to overlay the heatmap on the original test image, we need to upsample the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "img = test_image.squeeze()\n",
    "hmp = transforms.functional.resize(heatmap[None, None, ...], test_image.shape[2::], antialias=True).squeeze()\n",
    "\n",
    "f, ax = plt.subplots(1,3, figsize=(15, 15))\n",
    "\n",
    "ax[0].imshow(img, cmap=matplotlib.cm.gray)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('test image')\n",
    "\n",
    "ax[1].imshow(hmp, cmap=matplotlib.cm.bwr)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Grad-CAM heatmap')\n",
    "\n",
    "ax[2].imshow(hmp*img, cmap=matplotlib.cm.bwr)\n",
    "ax[2].axis('off')\n",
    "ax[2].set_title('multiplication')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM with VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to adapt our own model for Grad-CAM, in the following we will do this with a more powerful model trained on a much more difficult task than MNIST classification.\n",
    "\n",
    "Here, we will consider a VGG-16 model trained on ImageNet classification, inspired by this [blog post](https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://www.doc.ic.ac.uk/~bglocker/teaching/mli/gradcam.zip\n",
    "! unzip gradcam.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# use the ImageNet transformation\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# define the image dataset\n",
    "dataset = datasets.ImageFolder(root='./data/gradcam/', transform=transform)\n",
    "\n",
    "# define the dataloader to load that single image\n",
    "dataloader = data.DataLoader(dataset=dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "# ImageNet class names\n",
    "with open('./data/gradcam/imagenet1000_clsidx_to_labels.txt') as f:\n",
    "    idx2label = eval(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is print out of the VGG-16 architecture.\n",
    "```\n",
    "VGG(\n",
    "  (features): Sequential(\n",
    "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (3): ReLU(inplace=True)\n",
    "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (6): ReLU(inplace=True)\n",
    "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (8): ReLU(inplace=True)\n",
    "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (11): ReLU(inplace=True)\n",
    "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (13): ReLU(inplace=True)\n",
    "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (15): ReLU(inplace=True)\n",
    "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (18): ReLU(inplace=True)\n",
    "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (20): ReLU(inplace=True)\n",
    "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (22): ReLU(inplace=True)\n",
    "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (25): ReLU(inplace=True)\n",
    "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (27): ReLU(inplace=True)\n",
    "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (29): ReLU(inplace=True)\n",
    "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): Dropout(p=0.5, inplace=False)\n",
    "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "    (4): ReLU(inplace=True)\n",
    "    (5): Dropout(p=0.5, inplace=False)\n",
    "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output of the last convolutional layer is at index 29 in the `features` submodel (which is the convolutional part). Similar to before, we implement a modified VGG model to separate out the different parts of the model, so we can store the gradients of the selected convolutional layer during inference.\n",
    "\n",
    "This time, we recommend to separate out the model components alread in the `__init__` function, and then adjust the `forward` function accordingly.\n",
    "\n",
    "**Task:** Implement the modified VGG model for Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        # get the pretrained VGG16 network\n",
    "        self.vgg = vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # separate out the convolutional part up to the last max-pooling layer\n",
    "        \n",
    "        # disect the network to access its last convolutional layer\n",
    "        self.features_conv = self.vgg.features[:30]\n",
    "        \n",
    "        # get the max pool of the features stem\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # get the average pooling part\n",
    "        self.avg_pool = self.vgg.avgpool\n",
    "        \n",
    "        # get the classifier of the vgg19\n",
    "        self.classifier = self.vgg.classifier\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply the convolutional part\n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # register the hook\n",
    "        x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # apply the remaining max pooling\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        # apply the average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        \n",
    "        # apply the fully connected layers\n",
    "        x = x.view((1, -1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise our test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 0\n",
    "image = cv2.cvtColor(cv2.imread('./data/gradcam/test_images/image_' + str(image_index) + '.jpg'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('test image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run inference on the test image with out modified VGG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the VGG model\n",
    "vgg = VGG()\n",
    "\n",
    "# set the evaluation mode\n",
    "vgg.eval()\n",
    "\n",
    "# get the image from the dataloader\n",
    "# img, _ = next(iter(dataloader))\n",
    "img, _ = list(dataloader)[image_index]\n",
    "\n",
    "# get the most likely prediction of the model\n",
    "logits = vgg(img)\n",
    "\n",
    "predicted_label = logits.softmax(dim=1).argmax(dim=1)\n",
    "\n",
    "print(idx2label[predicted_label.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run Grad-CAM to generate the corresponding heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = run_gradcam(model=vgg, image=img, logits=logits, class_label=predicted_label, num_feature_channels=512)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap, cmap=matplotlib.cm.bwr)\n",
    "plt.colorbar()\n",
    "plt.title('Grad-CAM heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap will again have the same size as the output of the selected convolutional layer, which is 14x14 for the VGG-16.\n",
    "\n",
    "To visualise the heatmap and overlay it onto the test image, we use OpenCV for resizing, smoothing and generating a colour overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam = cv2.resize(heatmap.numpy(), (image.shape[1], image.shape[0]))\n",
    "gradcam = cv2.blur(gradcam,(50,50))\n",
    "gradcam = np.uint8(255 * gradcam)\n",
    "gradcam = cv2.cvtColor(cv2.applyColorMap(gradcam, cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "superimposed_img = cv2.addWeighted(gradcam, 0.5, image, 1.0, 0)\n",
    "\n",
    "f, ax = plt.subplots(1,3, figsize=(15, 15))\n",
    "\n",
    "ax[0].imshow(image)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('test image')\n",
    "\n",
    "ax[1].imshow(gradcam)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Grad-CAM heatmap')\n",
    "\n",
    "ax[2].imshow(superimposed_img)\n",
    "ax[2].axis('off')\n",
    "ax[2].set_title('overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional task:** Choose one of the [torchvision classification models](https://pytorch.org/vision/stable/models.html#classification) pre-trained on ImageNet and implement your own modified model in order to run Grad-CAM. Feel free to use other test images which can be found under `./data/gradcam/test_images/` (or add your own). Note, you will need to re-instantiate the dataset when changing the test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mli-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
