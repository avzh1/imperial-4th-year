{"cells":[{"cell_type":"markdown","metadata":{"id":"N_DDxV2TGQhV"},"source":["# Tutorial 0 - Introduction"]},{"cell_type":"markdown","metadata":{},"source":["In this tutorial, we will study different approaches for binary digit classification (two classes). We first explore a naive classification approach based on counting the number of \"white\" pixels. We then use a logistic regression where every pixel is a feature. Finally, we test a logistic regression using a small set of derived imaging features.\n","\n","**Objective:** Observe differences between three different classification approaches"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.data import random_split\n","from torchvision.datasets import MNIST\n","from torchvision import transforms\n","from sklearn import linear_model\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline"]},{"cell_type":"markdown","metadata":{},"source":["## Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! wget https://www.doc.ic.ac.uk/~bglocker/teaching/mli/morpho-mnist.zip\n","! unzip morpho-mnist.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qr6mTh1lGQhb"},"outputs":[],"source":["data_dir = './data'\n","train_set = MNIST(data_dir, train=True, download=True)\n","test_set = MNIST(data_dir, train=False, download=True)\n","print(train_set.data.shape)\n","print(test_set.data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xKpBDzcGQhd"},"outputs":[],"source":["def plot_digits(data, n_samples_row=10, colormap = matplotlib.cm.gray, clim=None):\n","    images = [image.reshape(28,28) for image in data]\n","    n_rows = (len(images) - 1) // n_samples_row + 1\n","    # append empty images if the last row is not complete\n","    empty_images = n_rows * n_samples_row - len(data)\n","    images.append(np.zeros((28, 28 * empty_images)))\n","    # draw row by row\n","    images_row = []\n","    for current_row in range(n_rows):\n","        tmp_row_images = images[current_row * n_samples_row : (current_row + 1) * n_samples_row]\n","        images_row.append(np.concatenate(tmp_row_images, axis=1))\n","    # draw all in one image\n","    image = np.concatenate(images_row, axis=0)\n","    plt.figure(figsize=(n_samples_row,n_rows))\n","    plt.imshow(image, cmap = colormap, clim=clim)\n","    plt.colorbar()\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's plot some random examples from the MNIST dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_digits(train_set.data[:100])\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Experiments"]},{"cell_type":"markdown","metadata":{},"source":["**Task:** Change the selected digits to [0,8] and other combinations, and observe the effect on the classification approaches."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selected_digits = [1,8]\n","\n","selected_indexes = np.isin(train_set.targets, selected_digits)\n","train_data = np.array(train_set.data[selected_indexes]).astype(np.float32) / 255.0\n","train_labels = np.array(train_set.targets[selected_indexes] == selected_digits[1])\n","\n","selected_indexes = np.isin(test_set.targets, selected_digits)\n","test_data = np.array(test_set.data[selected_indexes]).astype(np.float32) / 255.0\n","test_labels = np.array(test_set.targets[selected_indexes] == selected_digits[1])\n","\n","nsamples, nx, ny = train_data.shape\n","train_data = np.reshape(train_data,(nsamples,nx*ny))\n","\n","nsamples, nx, ny = test_data.shape\n","test_data = np.reshape(test_data,(nsamples,nx*ny)).astype(np.float32)"]},{"cell_type":"markdown","metadata":{},"source":["Let's plot some random examples from the training set with selected digit classes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_digits(train_data[:100])\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Approach 1: Classification via naive thresholding on hand-crafted feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvxNqYxZGQhe"},"outputs":[],"source":["sum_digit_0 = np.sum(train_data[train_labels==0]>0.5, axis=1)\n","sum_digit_1 = np.sum(train_data[train_labels==1]>0.5, axis=1)\n","\n","plt.hist(sum_digit_0, bins=50, range=(0.0, 250.0), fc=[1,0.5,0,0.5])\n","plt.hist(sum_digit_1, bins=50, range=(0.0, 250.0), fc=[0,0.5,1,0.5])\n","plt.legend(['zeros','eights'])\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Based on the histogram, let's choose a sensible threshold that separates the two classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kc3bzdfUGQhf"},"outputs":[],"source":["threshold = 85\n","sum_digit_pixels = np.sum(train_data>0.5, axis=1)\n","train_predictions = sum_digit_pixels >= threshold\n","\n","sum_digit_pixels = np.sum(test_data>0.5, axis=1)\n","test_predictions = sum_digit_pixels >= threshold"]},{"cell_type":"markdown","metadata":{},"source":["Calculate and report the classification accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOs13v-EGQhf"},"outputs":[],"source":["train_acc = 100.0 * (train_predictions == train_labels).mean()\n","print('Train accuracy = {:.2f}%'.format(train_acc))\n","\n","test_acc = 100.0 * (test_predictions == test_labels).mean()\n","print('Test accuracy = {:.2f}%'.format(test_acc))"]},{"cell_type":"markdown","metadata":{},"source":["### Approach 2: Logistic regression using all pixels as features"]},{"cell_type":"markdown","metadata":{},"source":["Let's use a logistic regression model from the scikit-learn library"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = linear_model.LogisticRegression(C=.0001, solver='liblinear')\n","model.fit(train_data, train_labels)"]},{"cell_type":"markdown","metadata":{},"source":["Calculate and report the classification accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_preds = model.predict(train_data)\n","logistic_train_acc = 100.0 * (train_preds == train_labels).mean()\n","print('Train accuracy = {:.2f}%'.format(logistic_train_acc))\n","\n","test_preds = model.predict(test_data)\n","logistic_test_acc = 100.0 * (test_preds == test_labels).mean()\n","print('Test accuracy = {:.2f}%'.format(logistic_test_acc))"]},{"cell_type":"markdown","metadata":{},"source":["Let's inspect the learned pattern"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vmax = np.max(np.abs(model.coef_)) / 2\n","plt.imshow(model.coef_.reshape(28,28), cmap=matplotlib.cm.bwr, clim=(-vmax,vmax))\n","plt.colorbar()\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pattern = np.tile(model.coef_, (100, 1))\n","plot_digits(np.multiply(train_data[:100],pattern), colormap=matplotlib.cm.bwr, clim=(-vmax,vmax))\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Approach 3: Logistic regression using image-derived features"]},{"cell_type":"markdown","metadata":{},"source":["We load the Morpho-MNIST dataset which contains pre-computed, image-derived features for MNIST digits"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set2 = pd.read_csv(data_dir + '/MNIST/train-morpho.csv')\n","train_set2['class label'] = train_set.targets\n","test_set2 = pd.read_csv(data_dir + '/MNIST/t10k-morpho.csv')\n","test_set2['class label'] = test_set.targets\n","\n","train_set2.head() # show the first five data entries of the training set"]},{"cell_type":"markdown","metadata":{},"source":["Let's inspect the feature separation across all digit classes for a random sample with 1,000 digits and two of the features. Feel free to try it out with other feature combinations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KR3irOV6GQhh"},"outputs":[],"source":["sns.set_theme(style=\"white\")\n","ax = sns.scatterplot(data=test_set2.sample(n=1000), x='area', y='length', hue='class label', alpha=0.8, marker='o', s=40, palette='tab10')\n","sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBdBPHQKGQhh"},"outputs":[],"source":["# Reformat the data\n","train_data2_all = np.transpose(np.array([train_set2['area'].values,train_set2['length'].values,train_set2['thickness'].values,train_set2['slant'].values,train_set2['width'].values,train_set2['height'].values]))\n","test_data2_all = np.transpose(np.array([test_set2['area'].values,test_set2['length'].values,test_set2['thickness'].values,test_set2['slant'].values,test_set2['width'].values,test_set2['height'].values]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5kuX8VYGQhh"},"outputs":[],"source":["selected_indexes = np.isin(train_set.targets, selected_digits)\n","train_data2 = np.array(train_data2_all[selected_indexes]).astype(np.float32)\n","train_labels2 = np.array(train_set.targets[selected_indexes] == selected_digits[1])\n","\n","selected_indexes = np.isin(test_set.targets, selected_digits)\n","test_data2 = np.array(test_data2_all[selected_indexes]).astype(np.float32)\n","test_labels2 = np.array(test_set.targets[selected_indexes] == selected_digits[1])"]},{"cell_type":"markdown","metadata":{},"source":["We normalise the features using feature scaling before feeding into the logistic regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe = make_pipeline(StandardScaler(), linear_model.LogisticRegression(C=.0001, solver='liblinear'))\n","pipe.fit(train_data2, train_labels2)"]},{"cell_type":"markdown","metadata":{},"source":["Calculate and report the classification accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_preds2 = pipe.predict(train_data2)\n","logistic_train_acc2 = 100.0 * (train_preds2 == train_labels2).mean()\n","print('Train accuracy = {:.2f}%'.format(logistic_train_acc2))\n","\n","test_preds2 = pipe.predict(test_data2)\n","logistic_test_acc2 = 100.0 * (test_preds2 == test_labels2).mean()\n","print('Test accuracy = {:.2f}%'.format(logistic_test_acc2))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"mli","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"0.0.0"},"vscode":{"interpreter":{"hash":"9655cee2dc85b65c074d3846acadf4c64fbe493f9a6d55dcea737f169d9a66be"}}},"nbformat":4,"nbformat_minor":0}
